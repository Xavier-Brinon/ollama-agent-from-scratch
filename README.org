#+title: Build an AI Agent from Scratch with Ollama
#+date: [2025-02-04 Tue]
#+startup: indent
#+property: header-args :results output
![CodeRabbit Pull Request Reviews](https://img.shields.io/coderabbit/prs/github/Xavier-Brinon/ollama-agent-from-scratch?utm_source=oss&utm_medium=github&utm_campaign=Xavier-Brinon%2Follama-agent-from-scratch&labelColor=171717&color=FF570A&link=https%3A%2F%2Fcoderabbit.ai&label=CodeRabbit+Reviews)

* üßô Ollama agent from scratch
Like the Front End Masters Course, [[frontendmasters.com/courses/ai-agents/][build an AI agent from scratch]] [fn:1], but locally
with Ollama.
* üèÅ Start a node project with TS
** Alias for the models
I can't be bothered typing the name of the models as they're downloaded as they
include versions and variations.
#+name: list models
#+begin_src bash
  ollama list
#+end_src

#+RESULTS: list models
: NAME                 ID              SIZE     MODIFIED
: llama3.3:latest      a6eb4748fd29    42 GB    6 days ago
: mistral-small:24b    573418a2cab9    14 GB    8 days ag
: deepseek-r1:70b      0c1615a8ca32    42 GB    10 days ago

Following [[https://github.com/ollama/ollama/blob/main/docs/modelfile.md][the modelfile]] doc,
One way to create the *alias* is to create a model file.
#+name: Modelfile
#+begin_src txt
FROM mistral-small:24b
#+end_src

Since I want to create the name *mistral* as the alias then type in the command:
#+begin_src bash
  ollama create mistral -f ~/Modelfile
  ollama run mistral
#+end_src

** Node app
Since the last version of *Node*, as of today, support TS, I'll just let the IDE
handle the types and run without transpiling.
In the root of the folder:
#+begin_src bash
  nvm install 23
  npm init -y
#+end_src

Some basic settings to begin with:
- Main entry point is =index.ts=. I write directly in typescript and let *Node*
  do the stripping. I will probably have to use only a subset of the TS features
  available.
- Tests will run with the built in test runner ~node --test~.
  *Node* will run against any file that ends with =.test.ts=


Because Typescript doesn't understand *Node* types, I need to also install them.
#+name: install node types
#+begin_src bash
  npm i -D @types/node
#+end_src
* üîë Env
It's all done locally, so no need of handling API Keys of any sort.

* üì¶ Ollama package
Based on the [[https://github.com/ollama/ollama-js][github page]]
And the [[https://ollama.com/][official site]]
** Models
The list in the [[https://ollama.com/search][search page]], download what your
machine can handle.
For me, I'll go with:
- [[https://ollama.com/library/deepseek-r1][deepseek-r1]]
- [[https://ollama.com/library/llama3.3][llama3.3]]
- [[https://ollama.com/library/mistral-small][mistral-small]]

** NPM package
Install the *Ollama-js* npm package
#+name: install mistral packagge
#+begin_src bash
  npm i ollama
#+end_src

** TODO Remove the package and work only with the fetch API.        :NoDeps:
** Reading inputs
There is an [[https://nodejs.org/docs/latest/api/readline.html#readline][example]] on *Node* documentation.

** TigerBeetle
I'm trying to follow some common sense based on the TigerBeetle [[https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md][manifesto]].
mainly the part on *Assertions*
#+begin_quote
Assert all function arguments and return values, pre/postconditions and
invariants. A function must not operate blindly on data it has not checked.
The purpose of a function is to increase the probability that a program is
correct.
Assertions within a function are part of how functions serve this purpose.
The assertion density of the code must average a minimum of two assertions per
function.
#+end_quote
** TODO Prompt can be passed via the cli                               :CLI:
Right now the command is ~node --run start~. It waits for the user input via
~readline.createInterface~.
Instead I want to be able to add the prompt when calling the LLM, something like
~node --run llm "this is the prompt"~
Checked [[https://nodejs.org/docs/latest/api/process.html#processargv][manual]], and I don't see yet how this can work when the command
triggers an npm command that calls the function:
1. npm run start
2. node index.ts


The ~process.argv~ returns the argv of (1.)
For now I'll get the prompt via the ~readline~ interface.
** TODO Read the article about the alternatives to enum            :ARTICLE:
Axel wrote an [[https://2ality.com/2025/01/typescript-enum-patterns.html][article]] about the Typescript ~enum~ and its alternatives.
** TODO Handle ~SIGTERM~
* Memory
I want to resend all the previous exchanges for follow up questions.
It will look like the LLM "remembers" what you just said.
The ~chat()~ function need to include the previous messages as context
for the current message.

* Footnotes

[fn:1]By [[https://frontendmasters.com/teachers/scott-moss/][Scott Moss]]
